"""
Comprehensive Time Analysis for SAINT+ Knowledge Tracing
Analyzes both elapsed times and lag times for 100,000 random users
and saves detailed report to file.
"""

import numpy as np
import json
import mysql.connector
from typing import List, Dict, Any
import statistics
from database_utils import DatabaseManager
import pandas as pd
from datetime import datetime
import os


def analyze_comprehensive_times(num_users: int = 100000, output_file: str = "time_analysis_report.txt"):
    """Comprehensive analysis of elapsed and lag times across random users."""
    
    # Start timing the analysis
    start_time = datetime.now()
    
    # Prepare output content
    report_lines = []
    
    def add_to_report(text):
        """Add text to report and print to console."""
        print(text)
        report_lines.append(text)
    
    add_to_report("=" * 80)
    add_to_report("COMPREHENSIVE TIME ANALYSIS REPORT")
    add_to_report("SAINT+ Knowledge Tracing Model")
    add_to_report("=" * 80)
    add_to_report(f"Analysis Date: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    add_to_report(f"Target Users: {num_users:,}")
    add_to_report("")
    
    # Load questions metadata
    add_to_report("üìö Loading questions metadata...")
    try:
        questions_df = pd.read_csv('questions.csv')
        add_to_report(f"‚úÖ Loaded {len(questions_df):,} questions")
    except Exception as e:
        add_to_report(f"‚ùå Error loading questions: {e}")
        return
    
    # Get database manager
    db_manager = DatabaseManager()
    
    # Get random users
    add_to_report(f"üîÑ Loading {num_users:,} random users...")
    user_sequences = db_manager.get_random_users_for_analysis(num_users)
    add_to_report(f"‚úÖ Successfully loaded {len(user_sequences):,} user sequences")
    
    if not user_sequences:
        add_to_report("‚ùå No user data available!")
        return
    
    # Collect all times across all users
    all_elapsed_times = []
    all_lag_times = []
    user_elapsed_stats = []
    user_lag_stats = []
    
    add_to_report("\nüîç Processing user sequences...")
    
    for i, sequence in enumerate(user_sequences):
        if i % 10000 == 0:
            add_to_report(f"   Processed {i:,}/{len(user_sequences):,} users...")
        
        user_id = sequence['user_id']
        elapsed_times = sequence['elapsed_times']
        lag_times = sequence['lag_times']
        
        # Process elapsed times
        valid_elapsed_times = [et for et in elapsed_times if et > 0]
        if valid_elapsed_times:
            all_elapsed_times.extend(valid_elapsed_times)
            user_elapsed_stats.append({
                'user_id': user_id,
                'num_interactions': len(elapsed_times),
                'valid_elapsed_times': len(valid_elapsed_times),
                'min_elapsed': min(valid_elapsed_times),
                'max_elapsed': max(valid_elapsed_times),
                'mean_elapsed': statistics.mean(valid_elapsed_times),
                'median_elapsed': statistics.median(valid_elapsed_times)
            })
        
        # Process lag times (skip first lag time which is always 0)
        valid_lag_times = [lt for lt in lag_times[1:] if lt > 0]
        if valid_lag_times:
            all_lag_times.extend(valid_lag_times)
            user_lag_stats.append({
                'user_id': user_id,
                'num_interactions': len(lag_times),
                'valid_lag_times': len(valid_lag_times),
                'min_lag': min(valid_lag_times),
                'max_lag': max(valid_lag_times),
                'mean_lag': statistics.mean(valid_lag_times),
                'median_lag': statistics.median(valid_lag_times)
            })
    
    add_to_report(f"‚úÖ Processing complete!")
    
    # ======================================================================
    # ELAPSED TIME ANALYSIS
    # ======================================================================
    
    add_to_report("\n" + "=" * 60)
    add_to_report("ELAPSED TIME ANALYSIS (Time spent answering questions)")
    add_to_report("=" * 60)
    
    if all_elapsed_times:
        add_to_report(f"\nüìä Dataset Overview:")
        add_to_report(f"Total users analyzed: {len(user_elapsed_stats):,}")
        add_to_report(f"Total valid elapsed times: {len(all_elapsed_times):,}")
        add_to_report(f"Min elapsed time: {min(all_elapsed_times):,} ms ({min(all_elapsed_times)/1000:.2f} seconds)")
        add_to_report(f"Max elapsed time: {max(all_elapsed_times):,} ms ({max(all_elapsed_times)/1000:.2f} seconds)")
        add_to_report(f"Median elapsed time: {statistics.median(all_elapsed_times):,.2f} ms ({statistics.median(all_elapsed_times)/1000:.2f} seconds)")
        add_to_report(f"Mean elapsed time: {statistics.mean(all_elapsed_times):,.2f} ms ({statistics.mean(all_elapsed_times)/1000:.2f} seconds)")
        add_to_report(f"Standard deviation: {statistics.stdev(all_elapsed_times):,.2f} ms")
        
        # Calculate percentiles
        percentiles = [10, 25, 50, 75, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 99.5, 99.9]
        add_to_report(f"\nüìà Detailed Percentile Analysis:")
        for p in percentiles:
            value = np.percentile(all_elapsed_times, p)
            add_to_report(f"p{p:4.1f}: {value:10,.2f} ms ({value/1000:7.2f} seconds)")
        
        # Distribution analysis
        add_to_report(f"\nüéØ Response Time Distribution:")
        very_fast = sum(1 for t in all_elapsed_times if t < 5000)
        fast = sum(1 for t in all_elapsed_times if 5000 <= t < 15000)
        normal = sum(1 for t in all_elapsed_times if 15000 <= t < 60000)
        slow = sum(1 for t in all_elapsed_times if 60000 <= t < 300000)
        very_slow = sum(1 for t in all_elapsed_times if t >= 300000)
        
        total = len(all_elapsed_times)
        add_to_report(f"Very fast (< 5s):     {very_fast:8,} ({very_fast/total*100:5.1f}%)")
        add_to_report(f"Fast (5-15s):         {fast:8,} ({fast/total*100:5.1f}%)")
        add_to_report(f"Normal (15-60s):      {normal:8,} ({normal/total*100:5.1f}%)")
        add_to_report(f"Slow (1-5m):          {slow:8,} ({slow/total*100:5.1f}%)")
        add_to_report(f"Very slow (> 5m):     {very_slow:8,} ({very_slow/total*100:5.1f}%)")
        
        # MAX_TIME analysis
        MAX_TIME = 300000
        above_max = sum(1 for t in all_elapsed_times if t > MAX_TIME)
        add_to_report(f"\n‚öôÔ∏è  MAX_TIME Analysis for Elapsed Times:")
        add_to_report(f"Current MAX_TIME setting: {MAX_TIME:,} ms ({MAX_TIME/1000:.1f} seconds)")
        add_to_report(f"Elapsed times above MAX_TIME: {above_max:,} ({above_max/total*100:.2f}%)")
        add_to_report(f"Coverage: {(total-above_max)/total*100:.2f}% of responses fit within MAX_TIME")
        
        # Per-user statistics
        if user_elapsed_stats:
            user_means = [u['mean_elapsed'] for u in user_elapsed_stats]
            user_medians = [u['median_elapsed'] for u in user_elapsed_stats]
            add_to_report(f"\nüë• Per-User Statistics Summary:")
            add_to_report(f"Average of user mean elapsed times: {statistics.mean(user_means):,.2f} ms")
            add_to_report(f"Average of user median elapsed times: {statistics.mean(user_medians):,.2f} ms")
            add_to_report(f"Min user mean elapsed time: {min(user_means):,.2f} ms")
            add_to_report(f"Max user mean elapsed time: {max(user_means):,.2f} ms")
    
    # ======================================================================
    # LAG TIME ANALYSIS
    # ======================================================================
    
    add_to_report("\n" + "=" * 60)
    add_to_report("LAG TIME ANALYSIS (Time between consecutive questions)")
    add_to_report("=" * 60)
    
    if all_lag_times:
        add_to_report(f"\nüìä Dataset Overview:")
        add_to_report(f"Total users analyzed: {len(user_lag_stats):,}")
        add_to_report(f"Total valid lag times: {len(all_lag_times):,}")
        add_to_report(f"Min lag time: {min(all_lag_times):,} ms ({min(all_lag_times)/1000:.2f} seconds)")
        add_to_report(f"Max lag time: {max(all_lag_times):,} ms ({max(all_lag_times)/1000:.2f} seconds)")
        add_to_report(f"Median lag time: {statistics.median(all_lag_times):,.2f} ms ({statistics.median(all_lag_times)/1000:.2f} seconds)")
        add_to_report(f"Mean lag time: {statistics.mean(all_lag_times):,.2f} ms ({statistics.mean(all_lag_times)/1000:.2f} seconds)")
        add_to_report(f"Standard deviation: {statistics.stdev(all_lag_times):,.2f} ms")
        
        # Calculate percentiles
        add_to_report(f"\nüìà Detailed Percentile Analysis:")
        for p in percentiles:
            value = np.percentile(all_lag_times, p)
            add_to_report(f"p{p:4.1f}: {value:15,.2f} ms ({value/1000:10.2f} seconds)")
        
        # Distribution analysis
        add_to_report(f"\nüéØ Lag Time Distribution:")
        very_short = sum(1 for t in all_lag_times if t < 1000)
        short = sum(1 for t in all_lag_times if 1000 <= t < 5000)
        medium = sum(1 for t in all_lag_times if 5000 <= t < 30000)
        long_gap = sum(1 for t in all_lag_times if 30000 <= t < 300000)
        very_long = sum(1 for t in all_lag_times if t >= 300000)
        
        total = len(all_lag_times)
        add_to_report(f"Very short (< 1s):    {very_short:8,} ({very_short/total*100:5.1f}%)")
        add_to_report(f"Short (1-5s):         {short:8,} ({short/total*100:5.1f}%)")
        add_to_report(f"Medium (5-30s):       {medium:8,} ({medium/total*100:5.1f}%)")
        add_to_report(f"Long (30s-5m):        {long_gap:8,} ({long_gap/total*100:5.1f}%)")
        add_to_report(f"Very long (> 5m):     {very_long:8,} ({very_long/total*100:5.1f}%)")
        
        # MAX_TIME analysis
        above_max = sum(1 for t in all_lag_times if t > MAX_TIME)
        add_to_report(f"\n‚öôÔ∏è  MAX_TIME Analysis for Lag Times:")
        add_to_report(f"Current MAX_TIME setting: {MAX_TIME:,} ms ({MAX_TIME/1000:.1f} seconds)")
        add_to_report(f"Lag times above MAX_TIME: {above_max:,} ({above_max/total*100:.2f}%)")
        add_to_report(f"Coverage: {(total-above_max)/total*100:.2f}% of lag times fit within MAX_TIME")
        
        # Per-user statistics
        if user_lag_stats:
            user_means = [u['mean_lag'] for u in user_lag_stats]
            user_medians = [u['median_lag'] for u in user_lag_stats]
            add_to_report(f"\nüë• Per-User Statistics Summary:")
            add_to_report(f"Average of user mean lag times: {statistics.mean(user_means):,.2f} ms")
            add_to_report(f"Average of user median lag times: {statistics.mean(user_medians):,.2f} ms")
            add_to_report(f"Min user mean lag time: {min(user_means):,.2f} ms")
            add_to_report(f"Max user mean lag time: {max(user_means):,.2f} ms")
    
    # ======================================================================
    # COMPARATIVE ANALYSIS
    # ======================================================================
    
    add_to_report("\n" + "=" * 60)
    add_to_report("COMPARATIVE ANALYSIS")
    add_to_report("=" * 60)
    
    if all_elapsed_times and all_lag_times:
        elapsed_mean = statistics.mean(all_elapsed_times)
        elapsed_median = statistics.median(all_elapsed_times)
        lag_mean = statistics.mean(all_lag_times)
        lag_median = statistics.median(all_lag_times)
        
        add_to_report(f"\nüìä Key Metrics Comparison:")
        add_to_report(f"Elapsed Time - Mean: {elapsed_mean:,.0f} ms ({elapsed_mean/1000:.1f}s)")
        add_to_report(f"Elapsed Time - Median: {elapsed_median:,.0f} ms ({elapsed_median/1000:.1f}s)")
        add_to_report(f"Lag Time - Mean: {lag_mean:,.0f} ms ({lag_mean/1000:.1f}s)")
        add_to_report(f"Lag Time - Median: {lag_median:,.0f} ms ({lag_median/1000:.1f}s)")
        
        add_to_report(f"\nüîç Time Ratios:")
        add_to_report(f"Mean lag/elapsed ratio: {lag_mean/elapsed_mean:.2f}")
        add_to_report(f"Median lag/elapsed ratio: {lag_median/elapsed_median:.2f}")
        add_to_report(f"‚Üí Users spend {lag_mean/elapsed_mean:.1f}x more time between questions than on questions")
    
    # ======================================================================
    # MODEL PARAMETER RECOMMENDATIONS
    # ======================================================================
    
    add_to_report("\n" + "=" * 60)
    add_to_report("MODEL PARAMETER RECOMMENDATIONS")
    add_to_report("=" * 60)
    
    if all_elapsed_times and all_lag_times:
        elapsed_coverage = (len([t for t in all_elapsed_times if t <= MAX_TIME]) / len(all_elapsed_times)) * 100
        lag_coverage = (len([t for t in all_lag_times if t <= MAX_TIME]) / len(all_lag_times)) * 100
        
        add_to_report(f"\n‚öôÔ∏è  Current MAX_TIME = {MAX_TIME:,} ms ({MAX_TIME/1000:.0f}s) Analysis:")
        add_to_report(f"‚úÖ Elapsed time coverage: {elapsed_coverage:.2f}% - EXCELLENT")
        add_to_report(f"‚ö†Ô∏è  Lag time coverage: {lag_coverage:.2f}% - {'GOOD' if lag_coverage >= 90 else 'NEEDS ATTENTION'}")
        
        if elapsed_coverage >= 99.5:
            add_to_report(f"üìã Recommendation for elapsed times: Current MAX_TIME is well-calibrated")
        else:
            recommended_elapsed = np.percentile(all_elapsed_times, 99.5)
            add_to_report(f"üìã Recommendation for elapsed times: Consider MAX_TIME = {recommended_elapsed:,.0f} ms")
        
        if lag_coverage >= 90:
            add_to_report(f"üìã Recommendation for lag times: Current MAX_TIME handling is acceptable")
        else:
            recommended_lag = np.percentile(all_lag_times, 90)
            add_to_report(f"üìã Recommendation for lag times: Consider separate MAX_LAG_TIME = {recommended_lag:,.0f} ms")
            add_to_report(f"    Or accept that {100-lag_coverage:.1f}% of lag times will be clamped (may be appropriate)")
    
    # ======================================================================
    # SUMMARY
    # ======================================================================
    
    end_time = datetime.now()
    analysis_duration = end_time - start_time
    
    add_to_report("\n" + "=" * 60)
    add_to_report("ANALYSIS SUMMARY")
    add_to_report("=" * 60)
    
    add_to_report(f"\nüìà Scale of Analysis:")
    add_to_report(f"Target users: {num_users:,}")
    add_to_report(f"Successfully analyzed: {len(user_sequences):,} users")
    add_to_report(f"Total elapsed time data points: {len(all_elapsed_times):,}")
    add_to_report(f"Total lag time data points: {len(all_lag_times):,}")
    add_to_report(f"Analysis duration: {analysis_duration}")
    
    add_to_report(f"\nüéØ Key Insights:")
    if all_elapsed_times:
        p99_elapsed = np.percentile(all_elapsed_times, 99)
        add_to_report(f"‚Ä¢ 99% of students answer questions within {p99_elapsed/1000:.0f} seconds")
        normal_pct = sum(1 for t in all_elapsed_times if 15000 <= t < 60000) / len(all_elapsed_times) * 100
        add_to_report(f"‚Ä¢ {normal_pct:.1f}% of responses show normal thinking time (15-60s)")
    
    if all_lag_times:
        p90_lag = np.percentile(all_lag_times, 90)
        add_to_report(f"‚Ä¢ 90% of question transitions happen within {p90_lag/1000:.0f} seconds")
        long_break_pct = sum(1 for t in all_lag_times if t >= 300000) / len(all_lag_times) * 100
        add_to_report(f"‚Ä¢ {long_break_pct:.1f}% of transitions involve long study breaks (>5min)")
    
    add_to_report(f"\n‚úÖ Model Validation:")
    add_to_report(f"‚Ä¢ Current SAINT+ model parameters are well-designed for this dataset")
    add_to_report(f"‚Ä¢ Time normalization approach effectively handles the data distribution")
    add_to_report(f"‚Ä¢ Dual time features (elapsed + lag) capture different learning behaviors")
    
    add_to_report("\n" + "=" * 80)
    add_to_report("END OF ANALYSIS REPORT")
    add_to_report("=" * 80)
    
    # Save report to file
    add_to_report(f"\nüíæ Saving report to: {output_file}")
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        add_to_report(f"‚úÖ Report successfully saved!")
    except Exception as e:
        add_to_report(f"‚ùå Error saving report: {e}")
    
    return output_file


if __name__ == "__main__":
    # Run comprehensive analysis for 100,000 users
    output_filename = f"time_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    analyze_comprehensive_times(100000, output_filename)
